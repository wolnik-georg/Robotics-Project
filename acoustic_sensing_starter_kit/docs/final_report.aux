\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wall2019morphological,wall2022passive,zoeller2020active}
\citation{zhang2025vibecheck}
\@writefile{toc}{\contentsline {section}{\numberline {I}INTRODUCTION}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {I-A}Research Questions}{1}{subsection.1.1}\protected@file@percent }
\citation{wall2019morphological}
\citation{wall2022passive}
\citation{zoeller2020active}
\citation{zhang2025vibecheck}
\@writefile{toc}{\contentsline {subsection}{\numberline {I-B}Contributions}{2}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}RELATED WORK}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {II-A}Acoustic Sensing for Soft Robotics}{2}{subsection.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {II-B}Robot Configuration Entanglement}{2}{subsection.2.2}\protected@file@percent }
\citation{wall2019morphological}
\@writefile{toc}{\contentsline {subsection}{\numberline {II-C}Research Gaps and Our Approach}{3}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}METHOD}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {III-A}Experimental Setup}{3}{subsection.3.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Test Objects and Workspace Configuration\relax }}{3}{table.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:objects}{{I}{3}{Test Objects and Workspace Configuration\relax }{table.caption.1}{}}
\citation{piczak2015environmental}
\citation{pedregosa2011scikit}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Single vs Multi-Sample Recording Protocol Validation (Binary Classification)\relax }}{4}{table.caption.2}\protected@file@percent }
\newlabel{tab:single_vs_multi}{{II}{4}{Single vs Multi-Sample Recording Protocol Validation (Binary Classification)\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-B}Feature Engineering}{4}{subsection.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Hand-Crafted Features vs.\ Spectrograms Comparison (Rotation 1: Train WS1+WS3, Validate WS2)\relax }}{4}{table.caption.3}\protected@file@percent }
\newlabel{tab:feature_comparison}{{III}{4}{Hand-Crafted Features vs.\ Spectrograms Comparison (Rotation 1: Train WS1+WS3, Validate WS2)\relax }{table.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-C}Classification Pipeline}{4}{subsection.3.3}\protected@file@percent }
\citation{pedregosa2011scikit}
\citation{pedregosa2011scikit}
\citation{mcfee2015librosa}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Hand-crafted acoustic feature architecture. We extract an 80-dimensional feature vector from each acoustic recording, comprising: 11 spectral features (centroid, rolloff, bandwidth, flatness, contrast), 39 MFCCs with first and second derivatives, 15 temporal features (zero-crossing rate, RMS energy, statistical moments), and 15 impulse response characteristics. This compact representation achieves 69.9\% cross-validation accuracy for 3-class classification (contact, no-contact, edge), outperforming high-dimensional mel-spectrograms (10,240D) which achieve 22--28\% validation accuracy (average 23.8\%).\relax }}{5}{figure.caption.4}\protected@file@percent }
\newlabel{fig:features}{{1}{5}{Hand-crafted acoustic feature architecture. We extract an 80-dimensional feature vector from each acoustic recording, comprising: 11 spectral features (centroid, rolloff, bandwidth, flatness, contrast), 39 MFCCs with first and second derivatives, 15 temporal features (zero-crossing rate, RMS energy, statistical moments), and 15 impulse response characteristics. This compact representation achieves 69.9\% cross-validation accuracy for 3-class classification (contact, no-contact, edge), outperforming high-dimensional mel-spectrograms (10,240D) which achieve 22--28\% validation accuracy (average 23.8\%).\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {III-D}Evaluation Strategy}{5}{subsection.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}EXPERIMENTAL RESULTS}{5}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-A}Proof of Concept: 3-Class Acoustic Contact Detection}{5}{subsection.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Classifier performance comparison: hand-crafted features vs.\ spectrograms on Rotation 1 validation (WS2). \textbf  {Left}: Hand-crafted features (80 dimensions) show moderate CV-validation gaps across all five classifiers, with Random Forest achieving best validation performance (33.9\%). \textbf  {Right}: Spectrograms (10,240 dimensions) exhibit severe overfitting with larger CV-validation gaps across all classifiers---Random Forest achieves 50.8\% CV but only 22.9\% validation (27.9pp gap). The consistently superior performance for hand-crafted features (5/5 classifiers, average +8.7pp advantage) confirms they extract more generalizable contact information. However, both representations fail to achieve strong cross-workspace generalization (features 0.98$\times $, spectrograms 0.71$\times $ over 33.3\% random baseline), revealing the dimensionality curse: 128$\times $ more parameters leads to worse performance.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:feature_comparison_conf}{{2}{6}{Classifier performance comparison: hand-crafted features vs.\ spectrograms on Rotation 1 validation (WS2). \textbf {Left}: Hand-crafted features (80 dimensions) show moderate CV-validation gaps across all five classifiers, with Random Forest achieving best validation performance (33.9\%). \textbf {Right}: Spectrograms (10,240 dimensions) exhibit severe overfitting with larger CV-validation gaps across all classifiers---Random Forest achieves 50.8\% CV but only 22.9\% validation (27.9pp gap). The consistently superior performance for hand-crafted features (5/5 classifiers, average +8.7pp advantage) confirms they extract more generalizable contact information. However, both representations fail to achieve strong cross-workspace generalization (features 0.98$\times $, spectrograms 0.71$\times $ over 33.3\% random baseline), revealing the dimensionality curse: 128$\times $ more parameters leads to worse performance.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Workspace rotation experimental strategy. Three rotations systematically evaluate position generalization: \textbf  {Rotation 1}: Train WS1+WS3 (7,290 samples), validate WS2 (1,338 samples). \textbf  {Rotation 2}: Train WS2+WS3 (7,290 samples), validate WS1 (1,362 samples). \textbf  {Rotation 3}: Train WS1+WS2 (8,028 samples), validate WS3 (1,215 samples). Each rotation uses balanced 3-class splits (contact, no-contact, edge) to test whether models generalize to workspaces with different surface geometries and robot configurations.\relax }}{6}{figure.caption.6}\protected@file@percent }
\newlabel{fig:experimental_setup}{{3}{6}{Workspace rotation experimental strategy. Three rotations systematically evaluate position generalization: \textbf {Rotation 1}: Train WS1+WS3 (7,290 samples), validate WS2 (1,338 samples). \textbf {Rotation 2}: Train WS2+WS3 (7,290 samples), validate WS1 (1,362 samples). \textbf {Rotation 3}: Train WS1+WS2 (8,028 samples), validate WS3 (1,215 samples). Each rotation uses balanced 3-class splits (contact, no-contact, edge) to test whether models generalize to workspaces with different surface geometries and robot configurations.\relax }{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Position Generalization Results (3 Workspace Rotations)\relax }}{7}{table.caption.9}\protected@file@percent }
\newlabel{tab:workspace_rotations}{{IV}{7}{Position Generalization Results (3 Workspace Rotations)\relax }{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-B}Position Generalization: Catastrophic Workspace-Dependent Failure}{7}{subsection.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-C}Object Generalization: Fundamental Limitation}{7}{subsection.4.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Object Generalization Results (3-Class): Training on Objects A, B, C (WS1+2+3) → Validation on Object D (WS4). Results reproduced across 5 seeds with std=0.0\%.\relax }}{7}{table.caption.10}\protected@file@percent }
\newlabel{tab:object_gen}{{V}{7}{Object Generalization Results (3-Class): Training on Objects A, B, C (WS1+2+3) → Validation on Object D (WS4). Results reproduced across 5 seeds with std=0.0\%.\relax }{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Object Generalization Results (Binary): Training on Objects A, B, C (WS1+2+3) → Validation on Object D (WS4). All classifiers collapse to exactly 50\% (std=0.0\%).\relax }}{7}{table.caption.11}\protected@file@percent }
\newlabel{tab:object_gen_binary}{{VI}{7}{Object Generalization Results (Binary): Training on Objects A, B, C (WS1+2+3) → Validation on Object D (WS4). All classifiers collapse to exactly 50\% (std=0.0\%).\relax }{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Proof of concept: 3-class acoustic contact detection using 80/20 train/test split on combined workspace data (WS1+WS2+WS3). \textbf  {Top row}: Objects A (cutout - balanced across all 3 classes) and B (empty workspace - pure no-contact). \textbf  {Bottom}: Object C (full contact - contact and edge only). Each object shows ground truth (left) and predictions (right) with confusion matrix. The model achieves high accuracy across all objects (89.81\%, 99.82\%, 90.17\%, average 93.3\%), validating that acoustic sensing enables 3-class contact state detection significantly above random baseline (33.3\%) for within-workspace scenarios. All confusion matrices display 3×3 grids showing the complete 3-class problem (contact, edge, no-contact). Color indicates contact state (contact=green, no-contact=red, edge=orange).\relax }}{8}{figure.caption.7}\protected@file@percent }
\newlabel{fig:reconstruction_proof}{{4}{8}{Proof of concept: 3-class acoustic contact detection using 80/20 train/test split on combined workspace data (WS1+WS2+WS3). \textbf {Top row}: Objects A (cutout - balanced across all 3 classes) and B (empty workspace - pure no-contact). \textbf {Bottom}: Object C (full contact - contact and edge only). Each object shows ground truth (left) and predictions (right) with confusion matrix. The model achieves high accuracy across all objects (89.81\%, 99.82\%, 90.17\%, average 93.3\%), validating that acoustic sensing enables 3-class contact state detection significantly above random baseline (33.3\%) for within-workspace scenarios. All confusion matrices display 3×3 grids showing the complete 3-class problem (contact, edge, no-contact). Color indicates contact state (contact=green, no-contact=red, edge=orange).\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Position generalization: 3-class reconstruction on Workspace 2 validation data (held out in Rotation 1). \textbf  {Top row}: Ground truth contact patterns for objects A (cutout - balanced across all 3 classes), B (empty workspace - pure no-contact), and C (full contact - contact and edge only). \textbf  {Bottom row}: Model predictions from acoustic features alone. Trained on WS1+WS3, the model achieves 55.7\% accuracy---moderate generalization that significantly exceeds random chance (33.3\%) but demonstrates substantial performance degradation from the 69.1\% cross-validation accuracy, revealing workspace-dependent acoustic signatures. Color indicates 3-class contact state (contact=green, no-contact=red, edge=orange).\relax }}{9}{figure.caption.8}\protected@file@percent }
\newlabel{fig:reconstruction_position}{{5}{9}{Position generalization: 3-class reconstruction on Workspace 2 validation data (held out in Rotation 1). \textbf {Top row}: Ground truth contact patterns for objects A (cutout - balanced across all 3 classes), B (empty workspace - pure no-contact), and C (full contact - contact and edge only). \textbf {Bottom row}: Model predictions from acoustic features alone. Trained on WS1+WS3, the model achieves 55.7\% accuracy---moderate generalization that significantly exceeds random chance (33.3\%) but demonstrates substantial performance degradation from the 69.1\% cross-validation accuracy, revealing workspace-dependent acoustic signatures. Color indicates 3-class contact state (contact=green, no-contact=red, edge=orange).\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Position vs Object Generalization Comparison (3-Class)\relax }}{10}{table.caption.12}\protected@file@percent }
\newlabel{tab:position_vs_object}{{VII}{10}{Position vs Object Generalization Comparison (3-Class)\relax }{table.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces 3-Class vs Binary Classification Comparison\relax }}{10}{table.caption.14}\protected@file@percent }
\newlabel{tab:binary_comparison}{{VIII}{10}{3-Class vs Binary Classification Comparison\relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-D}Comparison: 3-Class vs Binary Classification}{10}{subsection.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Object generalization on novel Object D (Workspace 4 holdout). \textbf  {Top}: Ground truth pattern for the geometrically distinct Object D. \textbf  {Bottom}: Model predictions achieving 41.7\% accuracy (Random Forest). Binary classification performs even worse at exactly 50\% (random chance). The scattered misclassifications demonstrate that most classifiers fail to generalize as different shapes produce non-overlapping eigenfrequency spectra. However, heavily-regularized GPU-MLP achieves 75.0\% validation (reproduced across 5 seeds with std=0.0\%), proving proper regularization (dropout 0.3, weight decay 0.01) enables geometry-invariant learning by preventing object-specific overfitting.\relax }}{11}{figure.caption.13}\protected@file@percent }
\newlabel{fig:reconstruction_holdout}{{6}{11}{Object generalization on novel Object D (Workspace 4 holdout). \textbf {Top}: Ground truth pattern for the geometrically distinct Object D. \textbf {Bottom}: Model predictions achieving 41.7\% accuracy (Random Forest). Binary classification performs even worse at exactly 50\% (random chance). The scattered misclassifications demonstrate that most classifiers fail to generalize as different shapes produce non-overlapping eigenfrequency spectra. However, heavily-regularized GPU-MLP achieves 75.0\% validation (reproduced across 5 seeds with std=0.0\%), proving proper regularization (dropout 0.3, weight decay 0.01) enables geometry-invariant learning by preventing object-specific overfitting.\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {IV-E}Physics-Based Interpretation: Object and Workspace Specificity}{11}{subsection.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}CONCLUSION}{12}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {V-A}Summary of Findings}{12}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {V-B}Contributions and Implications}{13}{subsection.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {V-C}Future Directions}{13}{subsection.5.3}\protected@file@percent }
\citation{zoeller2020active}
\bibstyle{IEEEtranN}
\bibdata{project_scientific_report}
\bibcite{wall2019morphological}{{1}{2019}{{Wall}}{{}}}
\bibcite{wall2022passive}{{2}{2022}{{Wall et~al.}}{{Wall, Z\"{o}ller, and Brock}}}
\bibcite{zoeller2020active}{{3}{2020}{{Z\"{o}ller et~al.}}{{Z\"{o}ller, Wall, and Brock}}}
\bibcite{zhang2025vibecheck}{{4}{2025}{{Zhang et~al.}}{{Zhang, Kim, Tang, Liang, He, et~al.}}}
\bibcite{piczak2015environmental}{{5}{2015}{{Piczak}}{{}}}
\bibcite{pedregosa2011scikit}{{6}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, et~al.}}}
\bibcite{mcfee2015librosa}{{7}{2015}{{McFee et~al.}}{{McFee, Raffel, Liang, Ellis, McVicar, Battenberg, and Nieto}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {V-D}Limitations}{14}{subsection.5.4}\protected@file@percent }
\gdef \@abspage@last{14}
